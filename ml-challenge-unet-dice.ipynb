{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"id":"T8xgNGBpGOpe","outputId":"345bd29c-c979-4488-86a4-cddd46c2b473","execution":{"iopub.status.busy":"2021-07-30T17:29:27.018167Z","iopub.execute_input":"2021-07-30T17:29:27.018507Z","iopub.status.idle":"2021-07-30T17:29:33.470016Z","shell.execute_reply.started":"2021-07-30T17:29:27.018465Z","shell.execute_reply":"2021-07-30T17:29:33.469050Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation-models-pytorch in /opt/conda/lib/python3.7/site-packages (0.2.0)\nRequirement already satisfied: efficientnet-pytorch==0.6.3 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.6.3)\nRequirement already satisfied: timm==0.4.12 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.4.12)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.7.4)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.8.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.7.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.61.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (8.2.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:29:33.473727Z","iopub.execute_input":"2021-07-30T17:29:33.474037Z","iopub.status.idle":"2021-07-30T17:29:39.976810Z","shell.execute_reply.started":"2021-07-30T17:29:33.474005Z","shell.execute_reply":"2021-07-30T17:29:39.975750Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset , DataLoader\nfrom torchvision import transforms as T\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport time \nfrom tqdm.notebook import tqdm\nfrom torchsummary import summary\nimport segmentation_models_pytorch as smp\n\ndevice =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"jUf886wiGby3","execution":{"iopub.status.busy":"2021-07-30T17:30:12.662246Z","iopub.execute_input":"2021-07-30T17:30:12.662579Z","iopub.status.idle":"2021-07-30T17:30:18.089126Z","shell.execute_reply.started":"2021-07-30T17:30:12.662544Z","shell.execute_reply":"2021-07-30T17:30:18.087952Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"os.mkdir('./AUG_DATA')\nos.mkdir('./AUG_DATA/train')\nos.mkdir('./AUG_DATA/masks')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:30:21.373962Z","iopub.execute_input":"2021-07-30T17:30:21.374302Z","iopub.status.idle":"2021-07-30T17:30:21.379006Z","shell.execute_reply.started":"2021-07-30T17:30:21.374267Z","shell.execute_reply":"2021-07-30T17:30:21.378126Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport glob\nimport cv2\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom skimage.transform import AffineTransform, warp\nfrom skimage import io, img_as_ubyte\nimport random\nimport os \nfrom scipy.ndimage import rotate\nfrom albumentations.augmentations.crops.transforms import CropNonEmptyMaskIfExists, RandomCrop\nimport albumentations as A\n\n\nIMAGES_TO_GENERATE = 1000\n\nimages_path = '../input/ml-challenge-data/image_chips'\nmask_path = '../input/ml-challenge-data/target_data'\n\nimg_aug_path = './AUG_DATA/train'\nmask_aug_path = './AUG_DATA/masks'\n\nimages= []\nmasks = []\n\nfor im in os.listdir(images_path):\n    images.append(os.path.join(images_path,im))\nfor msk in os.listdir(mask_path):\n    masks.append(os.path.join(mask_path,msk))\n\naug = A.Compose([\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.HorizontalFlip(p=1),\n    A.Transpose(p=1),\n    A.GridDistortion(p=1),\n    CropNonEmptyMaskIfExists(p=0.3,height=512,width=512),\n    A.geometric.rotate.Rotate (limit=90, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n])\n\ni=1\n\nwhile i<=IMAGES_TO_GENERATE:\n    number= random.randint(0,len(images)-1)\n    image = images[number]\n    mask = masks[number]\n    print(image,mask)\n    original_image =io.imread(image)\n    original_mask =io.imread(mask)    \n    augmented = aug(image=original_image,mask=original_mask)\n    transformed_image = augmented['image']\n    transformed_mask = augmented['mask']    \n    \n    new_image_path ='%s/%s.png'%(img_aug_path,i) \n    new_mask_path ='%s/%s.png'%(mask_aug_path,i) \n    print(new_image_path)\n    io.imsave(new_image_path,transformed_image)\n    io.imsave(new_mask_path,transformed_mask)\n    i=i+1","metadata":{"id":"4J-sdZJsqpj_","outputId":"4f5b797a-003f-4c99-c5c8-15d27d180405","execution":{"iopub.status.busy":"2021-07-30T17:30:51.699870Z","iopub.execute_input":"2021-07-30T17:30:51.700243Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H2613.jpg ../input/ml-challenge-data/target_data/H2613.jpg\n./AUG_DATA/train/1.png\n../input/ml-challenge-data/image_chips/H030.jpg ../input/ml-challenge-data/target_data/H030.jpg\n./AUG_DATA/train/2.png\n../input/ml-challenge-data/image_chips/H330.jpg ../input/ml-challenge-data/target_data/H330.jpg\n./AUG_DATA/train/3.png\n../input/ml-challenge-data/image_chips/J132.jpg ../input/ml-challenge-data/target_data/J132.jpg\n./AUG_DATA/train/4.png\n../input/ml-challenge-data/image_chips/H616.jpg ../input/ml-challenge-data/target_data/H616.jpg\n./AUG_DATA/train/5.png\n../input/ml-challenge-data/image_chips/G2712.jpg ../input/ml-challenge-data/target_data/G2712.jpg\n./AUG_DATA/train/6.png\n../input/ml-challenge-data/image_chips/G051.jpg ../input/ml-challenge-data/target_data/G051.jpg\n./AUG_DATA/train/7.png\n../input/ml-challenge-data/image_chips/H565.jpg ../input/ml-challenge-data/target_data/H565.jpg\n./AUG_DATA/train/8.png\n../input/ml-challenge-data/image_chips/H509.jpg ../input/ml-challenge-data/target_data/H509.jpg\n./AUG_DATA/train/9.png\n../input/ml-challenge-data/image_chips/6J02.jpg ../input/ml-challenge-data/target_data/6J02.jpg\n./AUG_DATA/train/10.png\n../input/ml-challenge-data/image_chips/6J01.jpg ../input/ml-challenge-data/target_data/6J01.jpg\n./AUG_DATA/train/11.png\n../input/ml-challenge-data/image_chips/J430.jpg ../input/ml-challenge-data/target_data/J430.jpg\n./AUG_DATA/train/12.png\n../input/ml-challenge-data/image_chips/G2675.jpg ../input/ml-challenge-data/target_data/G2675.jpg\n./AUG_DATA/train/13.png\n../input/ml-challenge-data/image_chips/H078.jpg ../input/ml-challenge-data/target_data/H078.jpg\n./AUG_DATA/train/14.png\n../input/ml-challenge-data/image_chips/H396.jpg ../input/ml-challenge-data/target_data/H396.jpg\n./AUG_DATA/train/15.png\n../input/ml-challenge-data/image_chips/H525.jpg ../input/ml-challenge-data/target_data/H525.jpg\n./AUG_DATA/train/16.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/17.png\n../input/ml-challenge-data/image_chips/J883.jpg ../input/ml-challenge-data/target_data/J883.jpg\n./AUG_DATA/train/18.png\n../input/ml-challenge-data/image_chips/H612.jpg ../input/ml-challenge-data/target_data/H612.jpg\n./AUG_DATA/train/19.png\n../input/ml-challenge-data/image_chips/J2520.jpg ../input/ml-challenge-data/target_data/J2520.jpg\n./AUG_DATA/train/20.png\n../input/ml-challenge-data/image_chips/G2578.jpg ../input/ml-challenge-data/target_data/G2578.jpg\n./AUG_DATA/train/21.png\n../input/ml-challenge-data/image_chips/4J02.jpg ../input/ml-challenge-data/target_data/4J02.jpg\n./AUG_DATA/train/22.png\n../input/ml-challenge-data/image_chips/H565.jpg ../input/ml-challenge-data/target_data/H565.jpg\n./AUG_DATA/train/23.png\n../input/ml-challenge-data/image_chips/J373.jpg ../input/ml-challenge-data/target_data/J373.jpg\n./AUG_DATA/train/24.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/25.png\n../input/ml-challenge-data/image_chips/J430.jpg ../input/ml-challenge-data/target_data/J430.jpg\n./AUG_DATA/train/26.png\n../input/ml-challenge-data/image_chips/J2513.jpg ../input/ml-challenge-data/target_data/J2513.jpg\n./AUG_DATA/train/27.png\n../input/ml-challenge-data/image_chips/H2840.jpg ../input/ml-challenge-data/target_data/H2840.jpg\n./AUG_DATA/train/28.png\n../input/ml-challenge-data/image_chips/J189.jpg ../input/ml-challenge-data/target_data/J189.jpg\n./AUG_DATA/train/29.png\n../input/ml-challenge-data/image_chips/G2653.jpg ../input/ml-challenge-data/target_data/G2653.jpg\n./AUG_DATA/train/30.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/30.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/4J02.jpg ../input/ml-challenge-data/target_data/4J02.jpg\n./AUG_DATA/train/31.png\n../input/ml-challenge-data/image_chips/J144.jpg ../input/ml-challenge-data/target_data/J144.jpg\n./AUG_DATA/train/32.png\n../input/ml-challenge-data/image_chips/H838.jpg ../input/ml-challenge-data/target_data/H838.jpg\n./AUG_DATA/train/33.png\n../input/ml-challenge-data/image_chips/H078.jpg ../input/ml-challenge-data/target_data/H078.jpg\n./AUG_DATA/train/34.png\n../input/ml-challenge-data/image_chips/J2522.jpg ../input/ml-challenge-data/target_data/J2522.jpg\n./AUG_DATA/train/35.png\n../input/ml-challenge-data/image_chips/H2708.jpg ../input/ml-challenge-data/target_data/H2708.jpg\n./AUG_DATA/train/36.png\n../input/ml-challenge-data/image_chips/1J01.jpg ../input/ml-challenge-data/target_data/1J01.jpg\n./AUG_DATA/train/37.png\n../input/ml-challenge-data/image_chips/L704.jpg ../input/ml-challenge-data/target_data/L704.jpg\n./AUG_DATA/train/38.png\n../input/ml-challenge-data/image_chips/G2712.jpg ../input/ml-challenge-data/target_data/G2712.jpg\n./AUG_DATA/train/39.png\n../input/ml-challenge-data/image_chips/J2520.jpg ../input/ml-challenge-data/target_data/J2520.jpg\n./AUG_DATA/train/40.png\n../input/ml-challenge-data/image_chips/G2567.jpg ../input/ml-challenge-data/target_data/G2567.jpg\n./AUG_DATA/train/41.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/41.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H330.jpg ../input/ml-challenge-data/target_data/H330.jpg\n./AUG_DATA/train/42.png\n../input/ml-challenge-data/image_chips/J454.jpg ../input/ml-challenge-data/target_data/J454.jpg\n./AUG_DATA/train/43.png\n../input/ml-challenge-data/image_chips/H2769.jpg ../input/ml-challenge-data/target_data/H2769.jpg\n./AUG_DATA/train/44.png\n../input/ml-challenge-data/image_chips/J2526.jpg ../input/ml-challenge-data/target_data/J2526.jpg\n./AUG_DATA/train/45.png\n../input/ml-challenge-data/image_chips/H2834.jpg ../input/ml-challenge-data/target_data/H2834.jpg\n./AUG_DATA/train/46.png\n../input/ml-challenge-data/image_chips/H289.jpg ../input/ml-challenge-data/target_data/H289.jpg\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/46.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"./AUG_DATA/train/47.png\n../input/ml-challenge-data/image_chips/G874.jpg ../input/ml-challenge-data/target_data/G874.jpg\n./AUG_DATA/train/48.png\n../input/ml-challenge-data/image_chips/J883.jpg ../input/ml-challenge-data/target_data/J883.jpg\n./AUG_DATA/train/49.png\n../input/ml-challenge-data/image_chips/J374.jpg ../input/ml-challenge-data/target_data/J374.jpg\n./AUG_DATA/train/50.png\n../input/ml-challenge-data/image_chips/H902.jpg ../input/ml-challenge-data/target_data/H902.jpg\n./AUG_DATA/train/51.png\n../input/ml-challenge-data/image_chips/J546.jpg ../input/ml-challenge-data/target_data/J546.jpg\n./AUG_DATA/train/52.png\n../input/ml-challenge-data/image_chips/J373.jpg ../input/ml-challenge-data/target_data/J373.jpg\n./AUG_DATA/train/53.png\n../input/ml-challenge-data/image_chips/H525.jpg ../input/ml-challenge-data/target_data/H525.jpg\n./AUG_DATA/train/54.png\n../input/ml-challenge-data/image_chips/G2772.jpg ../input/ml-challenge-data/target_data/G2772.jpg\n./AUG_DATA/train/55.png\n../input/ml-challenge-data/image_chips/J854.jpg ../input/ml-challenge-data/target_data/J854.jpg\n./AUG_DATA/train/56.png\n../input/ml-challenge-data/image_chips/J2515.jpg ../input/ml-challenge-data/target_data/J2515.jpg\n./AUG_DATA/train/57.png\n../input/ml-challenge-data/image_chips/J638.jpg ../input/ml-challenge-data/target_data/J638.jpg\n./AUG_DATA/train/58.png\n../input/ml-challenge-data/image_chips/H513.jpg ../input/ml-challenge-data/target_data/H513.jpg\n./AUG_DATA/train/59.png\n../input/ml-challenge-data/image_chips/G2567.jpg ../input/ml-challenge-data/target_data/G2567.jpg\n./AUG_DATA/train/60.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/60.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H327.jpg ../input/ml-challenge-data/target_data/H327.jpg\n./AUG_DATA/train/61.png\n../input/ml-challenge-data/image_chips/J495.jpg ../input/ml-challenge-data/target_data/J495.jpg\n./AUG_DATA/train/62.png\n../input/ml-challenge-data/image_chips/G2772.jpg ../input/ml-challenge-data/target_data/G2772.jpg\n./AUG_DATA/train/63.png\n../input/ml-challenge-data/image_chips/J2526.jpg ../input/ml-challenge-data/target_data/J2526.jpg\n./AUG_DATA/train/64.png\n../input/ml-challenge-data/image_chips/J332.jpg ../input/ml-challenge-data/target_data/J332.jpg\n./AUG_DATA/train/65.png\n../input/ml-challenge-data/image_chips/J430.jpg ../input/ml-challenge-data/target_data/J430.jpg\n./AUG_DATA/train/66.png\n../input/ml-challenge-data/image_chips/H2765.jpg ../input/ml-challenge-data/target_data/H2765.jpg\n./AUG_DATA/train/67.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/68.png\n../input/ml-challenge-data/image_chips/J854.jpg ../input/ml-challenge-data/target_data/J854.jpg\n./AUG_DATA/train/69.png\n../input/ml-challenge-data/image_chips/H838.jpg ../input/ml-challenge-data/target_data/H838.jpg\n./AUG_DATA/train/70.png\n../input/ml-challenge-data/image_chips/H166.jpg ../input/ml-challenge-data/target_data/H166.jpg\n./AUG_DATA/train/71.png\n../input/ml-challenge-data/image_chips/H2708.jpg ../input/ml-challenge-data/target_data/H2708.jpg\n./AUG_DATA/train/72.png\n../input/ml-challenge-data/image_chips/J629.jpg ../input/ml-challenge-data/target_data/J629.jpg\n./AUG_DATA/train/73.png\n../input/ml-challenge-data/image_chips/J637.jpg ../input/ml-challenge-data/target_data/J637.jpg\n./AUG_DATA/train/74.png\n../input/ml-challenge-data/image_chips/H525.jpg ../input/ml-challenge-data/target_data/H525.jpg\n./AUG_DATA/train/75.png\n../input/ml-challenge-data/image_chips/J754.jpg ../input/ml-challenge-data/target_data/J754.jpg\n./AUG_DATA/train/76.png\n../input/ml-challenge-data/image_chips/H220.jpg ../input/ml-challenge-data/target_data/H220.jpg\n./AUG_DATA/train/77.png\n../input/ml-challenge-data/image_chips/J430.jpg ../input/ml-challenge-data/target_data/J430.jpg\n./AUG_DATA/train/78.png\n../input/ml-challenge-data/image_chips/J586.jpg ../input/ml-challenge-data/target_data/J586.jpg\n./AUG_DATA/train/79.png\n../input/ml-challenge-data/image_chips/H122.jpg ../input/ml-challenge-data/target_data/H122.jpg\n./AUG_DATA/train/80.png\n../input/ml-challenge-data/image_chips/H220.jpg ../input/ml-challenge-data/target_data/H220.jpg\n./AUG_DATA/train/81.png\n../input/ml-challenge-data/image_chips/H108.jpg ../input/ml-challenge-data/target_data/H108.jpg\n./AUG_DATA/train/82.png\n../input/ml-challenge-data/image_chips/J854.jpg ../input/ml-challenge-data/target_data/J854.jpg\n./AUG_DATA/train/83.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/84.png\n../input/ml-challenge-data/image_chips/J2517.jpg ../input/ml-challenge-data/target_data/J2517.jpg\n./AUG_DATA/train/85.png\n../input/ml-challenge-data/image_chips/H509.jpg ../input/ml-challenge-data/target_data/H509.jpg\n./AUG_DATA/train/86.png\n../input/ml-challenge-data/image_chips/J374.jpg ../input/ml-challenge-data/target_data/J374.jpg\n./AUG_DATA/train/87.png\n../input/ml-challenge-data/image_chips/G2828.jpg ../input/ml-challenge-data/target_data/G2828.jpg\n./AUG_DATA/train/88.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/88.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/J132.jpg ../input/ml-challenge-data/target_data/J132.jpg\n./AUG_DATA/train/89.png\n../input/ml-challenge-data/image_chips/H243.jpg ../input/ml-challenge-data/target_data/H243.jpg\n./AUG_DATA/train/90.png\n../input/ml-challenge-data/image_chips/4J02.jpg ../input/ml-challenge-data/target_data/4J02.jpg\n./AUG_DATA/train/91.png\n../input/ml-challenge-data/image_chips/H078.jpg ../input/ml-challenge-data/target_data/H078.jpg\n./AUG_DATA/train/92.png\n../input/ml-challenge-data/image_chips/G2712.jpg ../input/ml-challenge-data/target_data/G2712.jpg\n./AUG_DATA/train/93.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/93.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/94.png\n../input/ml-challenge-data/image_chips/J638.jpg ../input/ml-challenge-data/target_data/J638.jpg\n./AUG_DATA/train/95.png\n../input/ml-challenge-data/image_chips/H696.jpg ../input/ml-challenge-data/target_data/H696.jpg\n./AUG_DATA/train/96.png\n../input/ml-challenge-data/image_chips/J332.jpg ../input/ml-challenge-data/target_data/J332.jpg\n./AUG_DATA/train/97.png\n../input/ml-challenge-data/image_chips/H388.jpg ../input/ml-challenge-data/target_data/H388.jpg\n./AUG_DATA/train/98.png\n../input/ml-challenge-data/image_chips/H585.jpg ../input/ml-challenge-data/target_data/H585.jpg\n./AUG_DATA/train/99.png\n../input/ml-challenge-data/image_chips/J854.jpg ../input/ml-challenge-data/target_data/J854.jpg\n./AUG_DATA/train/100.png\n../input/ml-challenge-data/image_chips/H2550.jpg ../input/ml-challenge-data/target_data/H2550.jpg\n./AUG_DATA/train/101.png\n../input/ml-challenge-data/image_chips/G2567.jpg ../input/ml-challenge-data/target_data/G2567.jpg\n./AUG_DATA/train/102.png\n../input/ml-challenge-data/image_chips/J495.jpg ../input/ml-challenge-data/target_data/J495.jpg\n./AUG_DATA/train/103.png\n../input/ml-challenge-data/image_chips/J2526.jpg ../input/ml-challenge-data/target_data/J2526.jpg\n./AUG_DATA/train/104.png\n../input/ml-challenge-data/image_chips/J373.jpg ../input/ml-challenge-data/target_data/J373.jpg\n./AUG_DATA/train/105.png\n../input/ml-challenge-data/image_chips/J185.jpg ../input/ml-challenge-data/target_data/J185.jpg\n./AUG_DATA/train/106.png\n../input/ml-challenge-data/image_chips/J2521.jpg ../input/ml-challenge-data/target_data/J2521.jpg\n./AUG_DATA/train/107.png\n../input/ml-challenge-data/image_chips/J185.jpg ../input/ml-challenge-data/target_data/J185.jpg\n./AUG_DATA/train/108.png\n../input/ml-challenge-data/image_chips/J2519.jpg ../input/ml-challenge-data/target_data/J2519.jpg\n./AUG_DATA/train/109.png\n../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/110.png\n../input/ml-challenge-data/image_chips/J2519.jpg ../input/ml-challenge-data/target_data/J2519.jpg\n./AUG_DATA/train/111.png\n../input/ml-challenge-data/image_chips/H585.jpg ../input/ml-challenge-data/target_data/H585.jpg\n./AUG_DATA/train/112.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/112.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H327.jpg ../input/ml-challenge-data/target_data/H327.jpg\n./AUG_DATA/train/113.png\n../input/ml-challenge-data/image_chips/J387.jpg ../input/ml-challenge-data/target_data/J387.jpg\n./AUG_DATA/train/114.png\n../input/ml-challenge-data/image_chips/J2521.jpg ../input/ml-challenge-data/target_data/J2521.jpg\n./AUG_DATA/train/115.png\n../input/ml-challenge-data/image_chips/J357.jpg ../input/ml-challenge-data/target_data/J357.jpg\n./AUG_DATA/train/116.png\n../input/ml-challenge-data/image_chips/J2515.jpg ../input/ml-challenge-data/target_data/J2515.jpg\n./AUG_DATA/train/117.png\n../input/ml-challenge-data/image_chips/H108.jpg ../input/ml-challenge-data/target_data/H108.jpg\n./AUG_DATA/train/118.png\n../input/ml-challenge-data/image_chips/6J01.jpg ../input/ml-challenge-data/target_data/6J01.jpg\n./AUG_DATA/train/119.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/120.png\n../input/ml-challenge-data/image_chips/H2765.jpg ../input/ml-challenge-data/target_data/H2765.jpg\n./AUG_DATA/train/121.png\n../input/ml-challenge-data/image_chips/H220.jpg ../input/ml-challenge-data/target_data/H220.jpg\n./AUG_DATA/train/122.png\n../input/ml-challenge-data/image_chips/H755.jpg ../input/ml-challenge-data/target_data/H755.jpg\n./AUG_DATA/train/123.png\n../input/ml-challenge-data/image_chips/H289.jpg ../input/ml-challenge-data/target_data/H289.jpg\n./AUG_DATA/train/124.png\n../input/ml-challenge-data/image_chips/H513.jpg ../input/ml-challenge-data/target_data/H513.jpg\n./AUG_DATA/train/125.png\n../input/ml-challenge-data/image_chips/7J01.jpg ../input/ml-challenge-data/target_data/7J01.jpg\n./AUG_DATA/train/126.png\n../input/ml-challenge-data/image_chips/H327.jpg ../input/ml-challenge-data/target_data/H327.jpg\n./AUG_DATA/train/127.png\n../input/ml-challenge-data/image_chips/H220.jpg ../input/ml-challenge-data/target_data/H220.jpg\n./AUG_DATA/train/128.png\n../input/ml-challenge-data/image_chips/J637.jpg ../input/ml-challenge-data/target_data/J637.jpg\n./AUG_DATA/train/129.png\n../input/ml-challenge-data/image_chips/J132.jpg ../input/ml-challenge-data/target_data/J132.jpg\n./AUG_DATA/train/130.png\n../input/ml-challenge-data/image_chips/H2613.jpg ../input/ml-challenge-data/target_data/H2613.jpg\n./AUG_DATA/train/131.png\n../input/ml-challenge-data/image_chips/H509.jpg ../input/ml-challenge-data/target_data/H509.jpg\n./AUG_DATA/train/132.png\n../input/ml-challenge-data/image_chips/J357.jpg ../input/ml-challenge-data/target_data/J357.jpg\n./AUG_DATA/train/133.png\n../input/ml-challenge-data/image_chips/H327.jpg ../input/ml-challenge-data/target_data/H327.jpg\n./AUG_DATA/train/134.png\n../input/ml-challenge-data/image_chips/J2515.jpg ../input/ml-challenge-data/target_data/J2515.jpg\n./AUG_DATA/train/135.png\n../input/ml-challenge-data/image_chips/4H23.jpg ../input/ml-challenge-data/target_data/4H23.jpg\n./AUG_DATA/train/136.png\n../input/ml-challenge-data/image_chips/J638.jpg ../input/ml-challenge-data/target_data/J638.jpg\n./AUG_DATA/train/137.png\n../input/ml-challenge-data/image_chips/J374.jpg ../input/ml-challenge-data/target_data/J374.jpg\n./AUG_DATA/train/138.png\n../input/ml-challenge-data/image_chips/G2567.jpg ../input/ml-challenge-data/target_data/G2567.jpg\n./AUG_DATA/train/139.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/139.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H2708.jpg ../input/ml-challenge-data/target_data/H2708.jpg\n./AUG_DATA/train/140.png\n../input/ml-challenge-data/image_chips/G051.jpg ../input/ml-challenge-data/target_data/G051.jpg\n./AUG_DATA/train/141.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/142.png\n../input/ml-challenge-data/image_chips/6J01.jpg ../input/ml-challenge-data/target_data/6J01.jpg\n./AUG_DATA/train/143.png\n../input/ml-challenge-data/image_chips/H585.jpg ../input/ml-challenge-data/target_data/H585.jpg\n./AUG_DATA/train/144.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/144.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H2550.jpg ../input/ml-challenge-data/target_data/H2550.jpg\n./AUG_DATA/train/145.png\n../input/ml-challenge-data/image_chips/4J02.jpg ../input/ml-challenge-data/target_data/4J02.jpg\n./AUG_DATA/train/146.png\n../input/ml-challenge-data/image_chips/J204.jpg ../input/ml-challenge-data/target_data/J204.jpg\n./AUG_DATA/train/147.png\n../input/ml-challenge-data/image_chips/H525.jpg ../input/ml-challenge-data/target_data/H525.jpg\n./AUG_DATA/train/148.png\n../input/ml-challenge-data/image_chips/J586.jpg ../input/ml-challenge-data/target_data/J586.jpg\n./AUG_DATA/train/149.png\n../input/ml-challenge-data/image_chips/J144.jpg ../input/ml-challenge-data/target_data/J144.jpg\n./AUG_DATA/train/150.png\n../input/ml-challenge-data/image_chips/H388.jpg ../input/ml-challenge-data/target_data/H388.jpg\n./AUG_DATA/train/151.png\n../input/ml-challenge-data/image_chips/H616.jpg ../input/ml-challenge-data/target_data/H616.jpg\n./AUG_DATA/train/152.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/153.png\n../input/ml-challenge-data/image_chips/H2765.jpg ../input/ml-challenge-data/target_data/H2765.jpg\n./AUG_DATA/train/154.png\n../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/155.png\n../input/ml-challenge-data/image_chips/4H23.jpg ../input/ml-challenge-data/target_data/4H23.jpg\n./AUG_DATA/train/156.png\n../input/ml-challenge-data/image_chips/J2526.jpg ../input/ml-challenge-data/target_data/J2526.jpg\n./AUG_DATA/train/157.png\n../input/ml-challenge-data/image_chips/J144.jpg ../input/ml-challenge-data/target_data/J144.jpg\n./AUG_DATA/train/158.png\n../input/ml-challenge-data/image_chips/H289.jpg ../input/ml-challenge-data/target_data/H289.jpg\n./AUG_DATA/train/159.png\n../input/ml-challenge-data/image_chips/J637.jpg ../input/ml-challenge-data/target_data/J637.jpg\n./AUG_DATA/train/160.png\n../input/ml-challenge-data/image_chips/H513.jpg ../input/ml-challenge-data/target_data/H513.jpg\n./AUG_DATA/train/161.png\n../input/ml-challenge-data/image_chips/H166.jpg ../input/ml-challenge-data/target_data/H166.jpg\n./AUG_DATA/train/162.png\n../input/ml-challenge-data/image_chips/H612.jpg ../input/ml-challenge-data/target_data/H612.jpg\n./AUG_DATA/train/163.png\n../input/ml-challenge-data/image_chips/J542.jpg ../input/ml-challenge-data/target_data/J542.jpg\n./AUG_DATA/train/164.png\n../input/ml-challenge-data/image_chips/H755.jpg ../input/ml-challenge-data/target_data/H755.jpg\n./AUG_DATA/train/165.png\n../input/ml-challenge-data/image_chips/4J02.jpg ../input/ml-challenge-data/target_data/4J02.jpg\n./AUG_DATA/train/166.png\n../input/ml-challenge-data/image_chips/6J02.jpg ../input/ml-challenge-data/target_data/6J02.jpg\n./AUG_DATA/train/167.png\n../input/ml-challenge-data/image_chips/H902.jpg ../input/ml-challenge-data/target_data/H902.jpg\n./AUG_DATA/train/168.png\n../input/ml-challenge-data/image_chips/K384.jpg ../input/ml-challenge-data/target_data/K384.jpg\n./AUG_DATA/train/169.png\n../input/ml-challenge-data/image_chips/J854.jpg ../input/ml-challenge-data/target_data/J854.jpg\n./AUG_DATA/train/170.png\n../input/ml-challenge-data/image_chips/G2712.jpg ../input/ml-challenge-data/target_data/G2712.jpg\n./AUG_DATA/train/171.png\n../input/ml-challenge-data/image_chips/L704.jpg ../input/ml-challenge-data/target_data/L704.jpg\n./AUG_DATA/train/172.png\n../input/ml-challenge-data/image_chips/J629.jpg ../input/ml-challenge-data/target_data/J629.jpg\n./AUG_DATA/train/173.png\n../input/ml-challenge-data/image_chips/H030.jpg ../input/ml-challenge-data/target_data/H030.jpg\n./AUG_DATA/train/174.png\n../input/ml-challenge-data/image_chips/H220.jpg ../input/ml-challenge-data/target_data/H220.jpg\n./AUG_DATA/train/175.png\n../input/ml-challenge-data/image_chips/J638.jpg ../input/ml-challenge-data/target_data/J638.jpg\n./AUG_DATA/train/176.png\n../input/ml-challenge-data/image_chips/H220.jpg ../input/ml-challenge-data/target_data/H220.jpg\n./AUG_DATA/train/177.png\n../input/ml-challenge-data/image_chips/H513.jpg ../input/ml-challenge-data/target_data/H513.jpg\n./AUG_DATA/train/178.png\n../input/ml-challenge-data/image_chips/H612.jpg ../input/ml-challenge-data/target_data/H612.jpg\n./AUG_DATA/train/179.png\n../input/ml-challenge-data/image_chips/G2828.jpg ../input/ml-challenge-data/target_data/G2828.jpg\n./AUG_DATA/train/180.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/180.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H755.jpg ../input/ml-challenge-data/target_data/H755.jpg\n./AUG_DATA/train/181.png\n../input/ml-challenge-data/image_chips/J854.jpg ../input/ml-challenge-data/target_data/J854.jpg\n./AUG_DATA/train/182.png\n../input/ml-challenge-data/image_chips/H2765.jpg ../input/ml-challenge-data/target_data/H2765.jpg\n./AUG_DATA/train/183.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/184.png\n../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/185.png\n../input/ml-challenge-data/image_chips/J180.jpg ../input/ml-challenge-data/target_data/J180.jpg\n./AUG_DATA/train/186.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/187.png\n../input/ml-challenge-data/image_chips/G2653.jpg ../input/ml-challenge-data/target_data/G2653.jpg\n./AUG_DATA/train/188.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/188.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H108.jpg ../input/ml-challenge-data/target_data/H108.jpg\n./AUG_DATA/train/189.png\n../input/ml-challenge-data/image_chips/H616.jpg ../input/ml-challenge-data/target_data/H616.jpg\n./AUG_DATA/train/190.png\n../input/ml-challenge-data/image_chips/H289.jpg ../input/ml-challenge-data/target_data/H289.jpg\n./AUG_DATA/train/191.png\n../input/ml-challenge-data/image_chips/J2513.jpg ../input/ml-challenge-data/target_data/J2513.jpg\n./AUG_DATA/train/192.png\n../input/ml-challenge-data/image_chips/J629.jpg ../input/ml-challenge-data/target_data/J629.jpg\n./AUG_DATA/train/193.png\n../input/ml-challenge-data/image_chips/H513.jpg ../input/ml-challenge-data/target_data/H513.jpg\n./AUG_DATA/train/194.png\n../input/ml-challenge-data/image_chips/H838.jpg ../input/ml-challenge-data/target_data/H838.jpg\n./AUG_DATA/train/195.png\n../input/ml-challenge-data/image_chips/H391.jpg ../input/ml-challenge-data/target_data/H391.jpg\n./AUG_DATA/train/196.png\n../input/ml-challenge-data/image_chips/H122.jpg ../input/ml-challenge-data/target_data/H122.jpg\n./AUG_DATA/train/197.png\n../input/ml-challenge-data/image_chips/J637.jpg ../input/ml-challenge-data/target_data/J637.jpg\n./AUG_DATA/train/198.png\n../input/ml-challenge-data/image_chips/J2522.jpg ../input/ml-challenge-data/target_data/J2522.jpg\n./AUG_DATA/train/199.png\n../input/ml-challenge-data/image_chips/J180.jpg ../input/ml-challenge-data/target_data/J180.jpg\n./AUG_DATA/train/200.png\n../input/ml-challenge-data/image_chips/J2519.jpg ../input/ml-challenge-data/target_data/J2519.jpg\n./AUG_DATA/train/201.png\n../input/ml-challenge-data/image_chips/J2517.jpg ../input/ml-challenge-data/target_data/J2517.jpg\n./AUG_DATA/train/202.png\n../input/ml-challenge-data/image_chips/J2526.jpg ../input/ml-challenge-data/target_data/J2526.jpg\n./AUG_DATA/train/203.png\n../input/ml-challenge-data/image_chips/L704.jpg ../input/ml-challenge-data/target_data/L704.jpg\n./AUG_DATA/train/204.png\n../input/ml-challenge-data/image_chips/7J01.jpg ../input/ml-challenge-data/target_data/7J01.jpg\n./AUG_DATA/train/205.png\n../input/ml-challenge-data/image_chips/H327.jpg ../input/ml-challenge-data/target_data/H327.jpg\n./AUG_DATA/train/206.png\n../input/ml-challenge-data/image_chips/6J02.jpg ../input/ml-challenge-data/target_data/6J02.jpg\n./AUG_DATA/train/207.png\n../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/208.png\n../input/ml-challenge-data/image_chips/H151.jpg ../input/ml-challenge-data/target_data/H151.jpg\n./AUG_DATA/train/209.png\n../input/ml-challenge-data/image_chips/H2552.jpg ../input/ml-challenge-data/target_data/H2552.jpg\n./AUG_DATA/train/210.png\n../input/ml-challenge-data/image_chips/J495.jpg ../input/ml-challenge-data/target_data/J495.jpg\n./AUG_DATA/train/211.png\n../input/ml-challenge-data/image_chips/H565.jpg ../input/ml-challenge-data/target_data/H565.jpg\n./AUG_DATA/train/212.png\n../input/ml-challenge-data/image_chips/4J01.jpg ../input/ml-challenge-data/target_data/4J01.jpg\n./AUG_DATA/train/213.png\n../input/ml-challenge-data/image_chips/J629.jpg ../input/ml-challenge-data/target_data/J629.jpg\n./AUG_DATA/train/214.png\n../input/ml-challenge-data/image_chips/J357.jpg ../input/ml-challenge-data/target_data/J357.jpg\n./AUG_DATA/train/215.png\n../input/ml-challenge-data/image_chips/H501.jpg ../input/ml-challenge-data/target_data/H501.jpg\n./AUG_DATA/train/216.png\n../input/ml-challenge-data/image_chips/J637.jpg ../input/ml-challenge-data/target_data/J637.jpg\n./AUG_DATA/train/217.png\n../input/ml-challenge-data/image_chips/G2675.jpg ../input/ml-challenge-data/target_data/G2675.jpg\n./AUG_DATA/train/218.png\n../input/ml-challenge-data/image_chips/J430.jpg ../input/ml-challenge-data/target_data/J430.jpg\n./AUG_DATA/train/219.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/220.png\n../input/ml-challenge-data/image_chips/G2675.jpg ../input/ml-challenge-data/target_data/G2675.jpg\n./AUG_DATA/train/221.png\n../input/ml-challenge-data/image_chips/J387.jpg ../input/ml-challenge-data/target_data/J387.jpg\n./AUG_DATA/train/222.png\n../input/ml-challenge-data/image_chips/H2841.jpg ../input/ml-challenge-data/target_data/H2841.jpg\n./AUG_DATA/train/223.png\n../input/ml-challenge-data/image_chips/H166.jpg ../input/ml-challenge-data/target_data/H166.jpg\n./AUG_DATA/train/224.png\n../input/ml-challenge-data/image_chips/J2517.jpg ../input/ml-challenge-data/target_data/J2517.jpg\n./AUG_DATA/train/225.png\n../input/ml-challenge-data/image_chips/J374.jpg ../input/ml-challenge-data/target_data/J374.jpg\n./AUG_DATA/train/226.png\n../input/ml-challenge-data/image_chips/H2765.jpg ../input/ml-challenge-data/target_data/H2765.jpg\n./AUG_DATA/train/227.png\n../input/ml-challenge-data/image_chips/J883.jpg ../input/ml-challenge-data/target_data/J883.jpg\n./AUG_DATA/train/228.png\n../input/ml-challenge-data/image_chips/G2653.jpg ../input/ml-challenge-data/target_data/G2653.jpg\n./AUG_DATA/train/229.png\n../input/ml-challenge-data/image_chips/H030.jpg ../input/ml-challenge-data/target_data/H030.jpg\n./AUG_DATA/train/230.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/231.png\n../input/ml-challenge-data/image_chips/K360.jpg ../input/ml-challenge-data/target_data/K360.jpg\n./AUG_DATA/train/232.png\n../input/ml-challenge-data/image_chips/J495.jpg ../input/ml-challenge-data/target_data/J495.jpg\n./AUG_DATA/train/233.png\n../input/ml-challenge-data/image_chips/H327.jpg ../input/ml-challenge-data/target_data/H327.jpg\n./AUG_DATA/train/234.png\n../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/235.png\n../input/ml-challenge-data/image_chips/6J01.jpg ../input/ml-challenge-data/target_data/6J01.jpg\n./AUG_DATA/train/236.png\n../input/ml-challenge-data/image_chips/J2874.jpg ../input/ml-challenge-data/target_data/J2874.jpg\n./AUG_DATA/train/237.png\n../input/ml-challenge-data/image_chips/H396.jpg ../input/ml-challenge-data/target_data/H396.jpg\n./AUG_DATA/train/238.png\n../input/ml-challenge-data/image_chips/H396.jpg ../input/ml-challenge-data/target_data/H396.jpg\n./AUG_DATA/train/239.png\n../input/ml-challenge-data/image_chips/H243.jpg ../input/ml-challenge-data/target_data/H243.jpg\n./AUG_DATA/train/240.png\n../input/ml-challenge-data/image_chips/G2578.jpg ../input/ml-challenge-data/target_data/G2578.jpg\n./AUG_DATA/train/241.png\n../input/ml-challenge-data/image_chips/G2828.jpg ../input/ml-challenge-data/target_data/G2828.jpg\n./AUG_DATA/train/242.png\n../input/ml-challenge-data/image_chips/J2520.jpg ../input/ml-challenge-data/target_data/J2520.jpg\n./AUG_DATA/train/243.png\n../input/ml-challenge-data/image_chips/J189.jpg ../input/ml-challenge-data/target_data/J189.jpg\n./AUG_DATA/train/244.png\n../input/ml-challenge-data/image_chips/G2675.jpg ../input/ml-challenge-data/target_data/G2675.jpg\n./AUG_DATA/train/245.png\n../input/ml-challenge-data/image_chips/J2521.jpg ../input/ml-challenge-data/target_data/J2521.jpg\n./AUG_DATA/train/246.png\n../input/ml-challenge-data/image_chips/J542.jpg ../input/ml-challenge-data/target_data/J542.jpg\n./AUG_DATA/train/247.png\n../input/ml-challenge-data/image_chips/J189.jpg ../input/ml-challenge-data/target_data/J189.jpg\n./AUG_DATA/train/248.png\n../input/ml-challenge-data/image_chips/4H23.jpg ../input/ml-challenge-data/target_data/4H23.jpg\n./AUG_DATA/train/249.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/250.png\n../input/ml-challenge-data/image_chips/J114.jpg ../input/ml-challenge-data/target_data/J114.jpg\n./AUG_DATA/train/251.png\n../input/ml-challenge-data/image_chips/J2513.jpg ../input/ml-challenge-data/target_data/J2513.jpg\n./AUG_DATA/train/252.png\n../input/ml-challenge-data/image_chips/J2526.jpg ../input/ml-challenge-data/target_data/J2526.jpg\n./AUG_DATA/train/253.png\n../input/ml-challenge-data/image_chips/4J01.jpg ../input/ml-challenge-data/target_data/4J01.jpg\n./AUG_DATA/train/254.png\n../input/ml-challenge-data/image_chips/H122.jpg ../input/ml-challenge-data/target_data/H122.jpg\n./AUG_DATA/train/255.png\n../input/ml-challenge-data/image_chips/H2613.jpg ../input/ml-challenge-data/target_data/H2613.jpg\n./AUG_DATA/train/256.png\n../input/ml-challenge-data/image_chips/J189.jpg ../input/ml-challenge-data/target_data/J189.jpg\n./AUG_DATA/train/257.png\n../input/ml-challenge-data/image_chips/J754.jpg ../input/ml-challenge-data/target_data/J754.jpg\n./AUG_DATA/train/258.png\n../input/ml-challenge-data/image_chips/J454.jpg ../input/ml-challenge-data/target_data/J454.jpg\n./AUG_DATA/train/259.png\n../input/ml-challenge-data/image_chips/J828.jpg ../input/ml-challenge-data/target_data/J828.jpg\n./AUG_DATA/train/260.png\n../input/ml-challenge-data/image_chips/G2653.jpg ../input/ml-challenge-data/target_data/G2653.jpg\n./AUG_DATA/train/261.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/261.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H2840.jpg ../input/ml-challenge-data/target_data/H2840.jpg\n./AUG_DATA/train/262.png\n../input/ml-challenge-data/image_chips/H434.jpg ../input/ml-challenge-data/target_data/H434.jpg\n./AUG_DATA/train/263.png\n../input/ml-challenge-data/image_chips/3J02.jpg ../input/ml-challenge-data/target_data/3J02.jpg\n./AUG_DATA/train/264.png\n../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/265.png\n../input/ml-challenge-data/image_chips/J357.jpg ../input/ml-challenge-data/target_data/J357.jpg\n./AUG_DATA/train/266.png\n../input/ml-challenge-data/image_chips/J189.jpg ../input/ml-challenge-data/target_data/J189.jpg\n./AUG_DATA/train/267.png\n../input/ml-challenge-data/image_chips/J2522.jpg ../input/ml-challenge-data/target_data/J2522.jpg\n./AUG_DATA/train/268.png\n../input/ml-challenge-data/image_chips/J637.jpg ../input/ml-challenge-data/target_data/J637.jpg\n./AUG_DATA/train/269.png\n../input/ml-challenge-data/image_chips/4H23.jpg ../input/ml-challenge-data/target_data/4H23.jpg\n./AUG_DATA/train/270.png\n../input/ml-challenge-data/image_chips/G2653.jpg ../input/ml-challenge-data/target_data/G2653.jpg\n./AUG_DATA/train/271.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/271.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/J114.jpg ../input/ml-challenge-data/target_data/J114.jpg\n./AUG_DATA/train/272.png\n../input/ml-challenge-data/image_chips/H612.jpg ../input/ml-challenge-data/target_data/H612.jpg\n./AUG_DATA/train/273.png\n../input/ml-challenge-data/image_chips/H289.jpg ../input/ml-challenge-data/target_data/H289.jpg\n./AUG_DATA/train/274.png\n../input/ml-challenge-data/image_chips/J132.jpg ../input/ml-challenge-data/target_data/J132.jpg\n./AUG_DATA/train/275.png\n../input/ml-challenge-data/image_chips/H2769.jpg ../input/ml-challenge-data/target_data/H2769.jpg\n./AUG_DATA/train/276.png\n../input/ml-challenge-data/image_chips/J332.jpg ../input/ml-challenge-data/target_data/J332.jpg\n./AUG_DATA/train/277.png\n../input/ml-challenge-data/image_chips/H363.jpg ../input/ml-challenge-data/target_data/H363.jpg\n./AUG_DATA/train/278.png\n../input/ml-challenge-data/image_chips/H388.jpg ../input/ml-challenge-data/target_data/H388.jpg\n./AUG_DATA/train/279.png\n../input/ml-challenge-data/image_chips/H030.jpg ../input/ml-challenge-data/target_data/H030.jpg\n./AUG_DATA/train/280.png\n../input/ml-challenge-data/image_chips/J2515.jpg ../input/ml-challenge-data/target_data/J2515.jpg\n./AUG_DATA/train/281.png\n../input/ml-challenge-data/image_chips/H108.jpg ../input/ml-challenge-data/target_data/H108.jpg\n./AUG_DATA/train/282.png\n../input/ml-challenge-data/image_chips/H151.jpg ../input/ml-challenge-data/target_data/H151.jpg\n./AUG_DATA/train/283.png\n../input/ml-challenge-data/image_chips/H2552.jpg ../input/ml-challenge-data/target_data/H2552.jpg\n./AUG_DATA/train/284.png\n../input/ml-challenge-data/image_chips/J365.jpg ../input/ml-challenge-data/target_data/J365.jpg\n./AUG_DATA/train/285.png\n../input/ml-challenge-data/image_chips/H616.jpg ../input/ml-challenge-data/target_data/H616.jpg\n./AUG_DATA/train/286.png\n../input/ml-challenge-data/image_chips/H501.jpg ../input/ml-challenge-data/target_data/H501.jpg\n./AUG_DATA/train/287.png\n../input/ml-challenge-data/image_chips/6J02.jpg ../input/ml-challenge-data/target_data/6J02.jpg\n./AUG_DATA/train/288.png\n../input/ml-challenge-data/image_chips/H141.jpg ../input/ml-challenge-data/target_data/H141.jpg\n./AUG_DATA/train/289.png\n../input/ml-challenge-data/image_chips/J2874.jpg ../input/ml-challenge-data/target_data/J2874.jpg\n./AUG_DATA/train/290.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/291.png\n../input/ml-challenge-data/image_chips/H108.jpg ../input/ml-challenge-data/target_data/H108.jpg\n./AUG_DATA/train/292.png\n../input/ml-challenge-data/image_chips/J189.jpg ../input/ml-challenge-data/target_data/J189.jpg\n./AUG_DATA/train/293.png\n../input/ml-challenge-data/image_chips/G2675.jpg ../input/ml-challenge-data/target_data/G2675.jpg\n./AUG_DATA/train/294.png\n../input/ml-challenge-data/image_chips/G874.jpg ../input/ml-challenge-data/target_data/G874.jpg\n./AUG_DATA/train/295.png\n../input/ml-challenge-data/image_chips/J180.jpg ../input/ml-challenge-data/target_data/J180.jpg\n./AUG_DATA/train/296.png\n../input/ml-challenge-data/image_chips/G2578.jpg ../input/ml-challenge-data/target_data/G2578.jpg\n./AUG_DATA/train/297.png\n../input/ml-challenge-data/image_chips/J365.jpg ../input/ml-challenge-data/target_data/J365.jpg\n./AUG_DATA/train/298.png\n../input/ml-challenge-data/image_chips/J754.jpg ../input/ml-challenge-data/target_data/J754.jpg\n./AUG_DATA/train/299.png\n../input/ml-challenge-data/image_chips/H616.jpg ../input/ml-challenge-data/target_data/H616.jpg\n./AUG_DATA/train/300.png\n../input/ml-challenge-data/image_chips/J2522.jpg ../input/ml-challenge-data/target_data/J2522.jpg\n./AUG_DATA/train/301.png\n../input/ml-challenge-data/image_chips/G051.jpg ../input/ml-challenge-data/target_data/G051.jpg\n./AUG_DATA/train/302.png\n../input/ml-challenge-data/image_chips/H525.jpg ../input/ml-challenge-data/target_data/H525.jpg\n./AUG_DATA/train/303.png\n../input/ml-challenge-data/image_chips/H612.jpg ../input/ml-challenge-data/target_data/H612.jpg\n./AUG_DATA/train/304.png\n../input/ml-challenge-data/image_chips/H509.jpg ../input/ml-challenge-data/target_data/H509.jpg\n./AUG_DATA/train/305.png\n../input/ml-challenge-data/image_chips/7J01.jpg ../input/ml-challenge-data/target_data/7J01.jpg\n./AUG_DATA/train/306.png\n../input/ml-challenge-data/image_chips/J2513.jpg ../input/ml-challenge-data/target_data/J2513.jpg\n./AUG_DATA/train/307.png\n../input/ml-challenge-data/image_chips/K360.jpg ../input/ml-challenge-data/target_data/K360.jpg\n./AUG_DATA/train/308.png\n../input/ml-challenge-data/image_chips/H2841.jpg ../input/ml-challenge-data/target_data/H2841.jpg\n./AUG_DATA/train/309.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/309.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/K384.jpg ../input/ml-challenge-data/target_data/K384.jpg\n./AUG_DATA/train/310.png\n../input/ml-challenge-data/image_chips/J2520.jpg ../input/ml-challenge-data/target_data/J2520.jpg\n./AUG_DATA/train/311.png\n../input/ml-challenge-data/image_chips/H2841.jpg ../input/ml-challenge-data/target_data/H2841.jpg\n./AUG_DATA/train/312.png\n../input/ml-challenge-data/image_chips/G2578.jpg ../input/ml-challenge-data/target_data/G2578.jpg\n./AUG_DATA/train/313.png\n../input/ml-challenge-data/image_chips/H243.jpg ../input/ml-challenge-data/target_data/H243.jpg\n./AUG_DATA/train/314.png\n../input/ml-challenge-data/image_chips/G2772.jpg ../input/ml-challenge-data/target_data/G2772.jpg\n./AUG_DATA/train/315.png\n../input/ml-challenge-data/image_chips/H696.jpg ../input/ml-challenge-data/target_data/H696.jpg\n./AUG_DATA/train/316.png\n../input/ml-challenge-data/image_chips/7J01.jpg ../input/ml-challenge-data/target_data/7J01.jpg\n./AUG_DATA/train/317.png\n../input/ml-challenge-data/image_chips/J185.jpg ../input/ml-challenge-data/target_data/J185.jpg\n./AUG_DATA/train/318.png\n../input/ml-challenge-data/image_chips/3J02.jpg ../input/ml-challenge-data/target_data/3J02.jpg\n./AUG_DATA/train/319.png\n../input/ml-challenge-data/image_chips/H006.jpg ../input/ml-challenge-data/target_data/H006.jpg\n./AUG_DATA/train/320.png\n../input/ml-challenge-data/image_chips/J204.jpg ../input/ml-challenge-data/target_data/J204.jpg\n./AUG_DATA/train/321.png\n../input/ml-challenge-data/image_chips/J365.jpg ../input/ml-challenge-data/target_data/J365.jpg\n./AUG_DATA/train/322.png\n../input/ml-challenge-data/image_chips/H2765.jpg ../input/ml-challenge-data/target_data/H2765.jpg\n./AUG_DATA/train/323.png\n../input/ml-challenge-data/image_chips/4H23.jpg ../input/ml-challenge-data/target_data/4H23.jpg\n./AUG_DATA/train/324.png\n../input/ml-challenge-data/image_chips/H2552.jpg ../input/ml-challenge-data/target_data/H2552.jpg\n./AUG_DATA/train/325.png\n../input/ml-challenge-data/image_chips/G799.jpg ../input/ml-challenge-data/target_data/G799.jpg\n./AUG_DATA/train/326.png\n../input/ml-challenge-data/image_chips/H902.jpg ../input/ml-challenge-data/target_data/H902.jpg\n./AUG_DATA/train/327.png\n../input/ml-challenge-data/image_chips/H2840.jpg ../input/ml-challenge-data/target_data/H2840.jpg\n./AUG_DATA/train/328.png\n../input/ml-challenge-data/image_chips/H141.jpg ../input/ml-challenge-data/target_data/H141.jpg\n./AUG_DATA/train/329.png\n../input/ml-challenge-data/image_chips/J144.jpg ../input/ml-challenge-data/target_data/J144.jpg\n./AUG_DATA/train/330.png\n../input/ml-challenge-data/image_chips/J2874.jpg ../input/ml-challenge-data/target_data/J2874.jpg\n./AUG_DATA/train/331.png\n../input/ml-challenge-data/image_chips/G2828.jpg ../input/ml-challenge-data/target_data/G2828.jpg\n./AUG_DATA/train/332.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/332.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H2517.jpg ../input/ml-challenge-data/target_data/H2517.jpg\n./AUG_DATA/train/333.png\n../input/ml-challenge-data/image_chips/H585.jpg ../input/ml-challenge-data/target_data/H585.jpg\n./AUG_DATA/train/334.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: ./AUG_DATA/masks/334.png is a low contrast image\n","output_type":"stream"},{"name":"stdout","text":"../input/ml-challenge-data/image_chips/H243.jpg ../input/ml-challenge-data/target_data/H243.jpg\n./AUG_DATA/train/335.png\n../input/ml-challenge-data/image_chips/H696.jpg ../input/ml-challenge-data/target_data/H696.jpg\n./AUG_DATA/train/336.png\n../input/ml-challenge-data/image_chips/G051.jpg ../input/ml-challenge-data/target_data/G051.jpg\n./AUG_DATA/train/337.png\n","output_type":"stream"}]},{"cell_type":"code","source":"images= \"./AUG_DATA/train/\"\ntargets= \"./AUG_DATA/masks/\"\n\nn_classes = 2\n\ndef _df_():\n  name = []\n  for dirname ,_ , filenames in os.walk(images):\n    for filename in filenames:\n      name.append(filename.split('.')[0])\n    \n    return pd.DataFrame({'id':name} , index = np.arange(0, len(name)))\n\ndf = _df_()\nprint(len(df))","metadata":{"id":"T-PJDPTXGeLJ","outputId":"451d6979-01e7-41b8-9ed2-7d348ce8f178","execution":{"iopub.status.busy":"2021-07-30T06:57:08.284625Z","iopub.execute_input":"2021-07-30T06:57:08.285177Z","iopub.status.idle":"2021-07-30T06:57:08.320627Z","shell.execute_reply.started":"2021-07-30T06:57:08.285101Z","shell.execute_reply":"2021-07-30T06:57:08.318467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_val = train_test_split(df['id'].values , test_size\n                                   =0.25, random_state =19)\nprint(f'Train : {len(X_train)}')\nprint(f'Val : {len(X_val)}')","metadata":{"id":"_toQUgmxGsIg","outputId":"23511843-57a8-44e4-c9b1-4026555bca81","execution":{"iopub.status.busy":"2021-07-30T06:57:32.786655Z","iopub.execute_input":"2021-07-30T06:57:32.787395Z","iopub.status.idle":"2021-07-30T06:57:32.80467Z","shell.execute_reply.started":"2021-07-30T06:57:32.787341Z","shell.execute_reply":"2021-07-30T06:57:32.800432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(images + df['id'][0] + '.png')\ntarg = Image.open(targets + df['id'][0] + '.png')\n\nprint('Image Size', np.asarray(img).shape)\nprint('Mask Size' , np.asarray(targ).shape)\n\nplt.figure(figsize=(10,10))\nplt.imshow(img)\nplt.imshow(targ , alpha =0.4)\nplt.show()","metadata":{"id":"0bfVCtZHHHzy","outputId":"21865047-c985-439d-91ee-5afc30cbacf4","execution":{"iopub.status.busy":"2021-07-30T06:57:36.760457Z","iopub.execute_input":"2021-07-30T06:57:36.761095Z","iopub.status.idle":"2021-07-30T06:57:37.449435Z","shell.execute_reply.started":"2021-07-30T06:57:36.761013Z","shell.execute_reply":"2021-07-30T06:57:37.447899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dense_target(tar: np.ndarray):\n    classes =np.unique(tar)\n    dummy= np.zeros_like(tar)\n    for idx , value in enumerate(classes):\n        mask = np.where(tar == value)\n        dummy[mask] = idx\n    return dummy\n\nclass SegData(Dataset):\n\n  def __init__(self , image_path , target_path , X , mean , std , transform =None , test=False):\n    self.image_path = image_path\n    self.target_path = target_path\n    self.X = X\n    self.transform =transform\n    self.mean = mean\n    self.std = std\n    self.test =test\n\n  def __len__(self):\n    return len(self.X)\n  \n  def __getitem__(self, idx):\n    img = cv2.cvtColor(cv2.imread(self.image_path + self.X[idx] + '.png') , cv2.COLOR_BGR2RGB)\n    target = cv2.imread(self.target_path + self.X[idx] + '.png' , cv2.IMREAD_GRAYSCALE)\n    kernel_sharp = np.array(([-2, -2, -2], [-2, 17, -2], [-2, -2, -2]), dtype='int')\n    img = cv2.filter2D(img, -1, kernel_sharp)\n    target = cv2.filter2D(target, -1, kernel_sharp)\n    img = cv2.resize(img, (512 , 512) , interpolation = cv2.INTER_NEAREST)\n    target = cv2.resize(target , (512 , 512), interpolation = cv2.INTER_NEAREST)\n    target = np.where( target > 0,255,0)\n  \n    if self.transform is not None:\n      aug = self.transform(image = img , target = target )\n      img = Image.fromarray(aug['image'])\n      target = aug['target']\n    \n    if self.transform is None:\n      img = Image.fromarray(img) \n    \n    t = T.Compose([T.ToTensor() , T.Normalize(self.mean , self.std)])\n    \n    if self.test is False:\n      img = t(img)\n    target = dense_target(target)\n    target = torch.from_numpy(target).long()\n    return img ,target","metadata":{"id":"fGT_Zi0tHXga","execution":{"iopub.status.busy":"2021-07-30T06:57:44.169748Z","iopub.execute_input":"2021-07-30T06:57:44.170608Z","iopub.status.idle":"2021-07-30T06:57:44.194879Z","shell.execute_reply.started":"2021-07-30T06:57:44.17055Z","shell.execute_reply":"2021-07-30T06:57:44.192777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = [0.485 ,0.456 ,0.406]\nstd = [0.229 , 0.224 , 0.225]\n\ntrain_set = SegData(images, targets, X_train , mean, std)\nval_set = SegData(images , targets , X_val , mean , std)\n\nbatch_size = 4\ntrain_loader= DataLoader(train_set , batch_size= batch_size , shuffle =True)\nval_loader = DataLoader(val_set , batch_size = batch_size , shuffle =True)","metadata":{"id":"h1uJeBB9ISn1","execution":{"iopub.status.busy":"2021-07-30T06:57:46.652638Z","iopub.execute_input":"2021-07-30T06:57:46.653113Z","iopub.status.idle":"2021-07-30T06:57:46.662767Z","shell.execute_reply.started":"2021-07-30T06:57:46.65308Z","shell.execute_reply":"2021-07-30T06:57:46.661082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x , y =next(iter(train_loader))\n\nprint(f' x = shape : {x.shape} ; type :{x.dtype}')\nprint(f' x = min : {x.min()} ; max : {x.max()}')\nprint(f' y = shape: {y.shape}; class : {y.unique()}; type: {y.dtype}')","metadata":{"id":"V60iAn6LIU2E","outputId":"d7e4c6ba-9c93-4f54-b42c-0d074dba1952","execution":{"iopub.status.busy":"2021-07-30T06:57:48.869872Z","iopub.execute_input":"2021-07-30T06:57:48.870273Z","iopub.status.idle":"2021-07-30T06:57:49.272056Z","shell.execute_reply.started":"2021-07-30T06:57:48.870221Z","shell.execute_reply":"2021-07-30T06:57:49.271021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = smp.UnetPlusPlus('resnet50',encoder_weights='imagenet', classes = 2, activation=None,\n                 encoder_depth= 5, decoder_channels=[256,128, 64, 32,16])\nmodel=model.to(device)","metadata":{"id":"99-eIg16IXvu","outputId":"d6f952cb-3a8e-416e-f4fb-6bd4fbffc72f","execution":{"iopub.status.busy":"2021-07-30T06:57:54.820905Z","iopub.execute_input":"2021-07-30T06:57:54.821513Z","iopub.status.idle":"2021-07-30T06:58:10.361785Z","shell.execute_reply.started":"2021-07-30T06:57:54.821465Z","shell.execute_reply":"2021-07-30T06:58:10.360479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model, input_size=(3, 512 , 512))","metadata":{"id":"GPWRluzOIrjb","outputId":"2eae2b7d-090d-4be1-fcc7-a1c1ca7c2126","execution":{"iopub.status.busy":"2021-07-30T06:58:15.010027Z","iopub.execute_input":"2021-07-30T06:58:15.010541Z","iopub.status.idle":"2021-07-30T06:58:16.647428Z","shell.execute_reply.started":"2021-07-30T06:58:15.010504Z","shell.execute_reply":"2021-07-30T06:58:16.646411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pixel_wise_accuracy(output , mask):\n  with torch.no_grad():\n    output = torch.argmax(F.softmax(output , dim =1) , dim=1)\n    correct = torch.eq(output , mask).int()\n    accuracy = float(correct.sum())/ float(correct.numel())#total number\n  return accuracy","metadata":{"id":"q_I6z-V7I5XI","execution":{"iopub.status.busy":"2021-07-30T06:58:23.003982Z","iopub.execute_input":"2021-07-30T06:58:23.004432Z","iopub.status.idle":"2021-07-30T06:58:23.012527Z","shell.execute_reply.started":"2021-07-30T06:58:23.004374Z","shell.execute_reply":"2021-07-30T06:58:23.010746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IoU(pred , true_pred , smooth =1e-10 , n_classes=2):\n  with torch.no_grad():\n    pred = torch.argmax(F.softmax(pred , dim =1) , dim=1)\n    pred = pred.contiguous().view(-1)\n    true_pred = true_pred.contiguous().view(-1)\n\n    iou_class = []\n    for value in range(0, n_classes):\n      true_class = pred == value\n      true_label = true_pred == value\n\n      if true_label.long().sum().item()==0:\n        iou_class.append(np.nan)\n        \n      else:\n    \n        inter = torch.logical_and(true_class, true_label).sum().float().item()\n        union = torch.logical_or(true_class , true_label).sum().float().item()\n\n        iou = (inter + smooth)/(union + smooth)\n        iou_class.append(iou)\n\n    return np.nanmean(iou_class)","metadata":{"id":"cu4G3MsEI9y_","execution":{"iopub.status.busy":"2021-07-30T06:58:29.052228Z","iopub.execute_input":"2021-07-30T06:58:29.052693Z","iopub.status.idle":"2021-07-30T06:58:29.073042Z","shell.execute_reply.started":"2021-07-30T06:58:29.052653Z","shell.execute_reply":"2021-07-30T06:58:29.068884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_loss(true, logits, eps=1e-7):\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return (1 - jacc_loss)","metadata":{"id":"gHh1AE5dWSj9","execution":{"iopub.status.busy":"2021-07-30T06:58:31.817726Z","iopub.execute_input":"2021-07-30T06:58:31.818267Z","iopub.status.idle":"2021-07-30T06:58:31.829952Z","shell.execute_reply.started":"2021-07-30T06:58:31.818189Z","shell.execute_reply":"2021-07-30T06:58:31.828754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DiceBceLoss(true, logits, eps=1e-7):\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    dice_loss = 1- ((2.*intersection + eps)/(cardinality + eps)).mean()\n    bce = F.cross_entropy(logits, true , reduction =\"mean\")\n    dice_bce = bce + dice_loss\n    return dice_bce","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:58:34.552384Z","iopub.execute_input":"2021-07-30T06:58:34.552753Z","iopub.status.idle":"2021-07-30T06:58:34.564945Z","shell.execute_reply.started":"2021-07-30T06:58:34.552721Z","shell.execute_reply":"2021-07-30T06:58:34.563893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit(epochs, model, train_loader, val_loader, optimizer, scheduler, patch=False):\n    train_losses = []\n    test_losses = []\n    val_iou = []; val_acc = []\n    train_iou = []; train_acc = []\n    lrs = []\n    min_loss = np.inf\n    decrease = 1 ; not_improve=0\n\n    model.to(device)\n    fit_time = time.time()\n    for e in range(epochs):\n        since = time.time()\n        running_loss = 0\n        iou_score = 0\n        accuracy = 0\n        #training loop\n        model.train()\n        for i, data in enumerate(tqdm(train_loader)):\n            #training phase\n            image_tiles, mask_tiles = data\n            if patch:\n                bs, n_tiles, c, h, w = image_tiles.size()\n\n                image_tiles = image_tiles.view(-1,c, h, w)\n                mask_tiles = mask_tiles.view(-1, h, w)\n            \n            image = image_tiles.to(device); mask = mask_tiles.to(device);\n            #forward\n            output = model(image)\n            loss = DiceBceLoss(mask, output)\n            #evaluation metrics\n            iou_score += IoU(output, mask)\n            accuracy += pixel_wise_accuracy(output, mask)\n            #backward\n            loss.backward()\n            optimizer.step() #update weight          \n            optimizer.zero_grad() #reset gradient\n            \n            #step the learning rate\n            lrs.append(get_lr(optimizer))\n            scheduler.step() \n            \n            running_loss += loss.item()\n            \n        else:\n            model.eval()\n            test_loss = 0\n            test_accuracy = 0\n            val_iou_score = 0\n            #validation loop\n            with torch.no_grad():\n                for i, data in enumerate(tqdm(val_loader)):\n                    #reshape to 9 patches from single image, delete batch size\n                    image_tiles, mask_tiles = data\n\n                    if patch:\n                        bs, n_tiles, c, h, w = image_tiles.size()\n\n                        image_tiles = image_tiles.view(-1,c, h, w)\n                        mask_tiles = mask_tiles.view(-1, h, w)\n                    \n                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n                    output = model(image)\n                    #evaluation metrics\n                    val_iou_score +=  IoU(output, mask)\n                    test_accuracy += pixel_wise_accuracy(output, mask)\n                    #loss\n                    loss = jaccard_loss(mask, output)                                  \n                    test_loss += loss.item()\n            \n            #calculatio mean for each batch\n            train_losses.append(running_loss/len(train_loader))\n            test_losses.append(test_loss/len(val_loader))\n\n\n            if min_loss > (test_loss/len(val_loader)):\n                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n                min_loss = (test_loss/len(val_loader))\n                decrease += 1\n                print('saving model...')\n                torch.save(model, 'model-{:.3f}.pt'.format(val_iou_score/len(val_loader)))\n                    \n\n            # if (test_loss/len(val_loader)) > min_loss:\n            #     not_improve += 1\n            #     min_loss = (test_loss/len(val_loader))\n            #     print(f'Loss did not  Decrease for {not_improve} time')\n            #     if not_improve == 7:\n            #         print('Loss did not decrease for the 7th time , Stop Training')\n            #         break\n            \n            #iou\n            val_iou.append(val_iou_score/len(val_loader))\n            train_iou.append(iou_score/len(train_loader))\n            train_acc.append(accuracy/len(train_loader))\n            val_acc.append(test_accuracy/ len(val_loader))\n            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n                  \"Train IoU:{:.3f}..\".format(iou_score/len(train_loader)),\n                  \"Val IoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n        \n    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n               'train_miou' :train_iou, 'val_miou':val_iou,\n               'train_acc' :train_acc, 'val_acc':val_acc,\n               'lrs': lrs}\n    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n    return history","metadata":{"id":"4MXWYUlLI_ye","execution":{"iopub.status.busy":"2021-07-30T06:58:37.234599Z","iopub.execute_input":"2021-07-30T06:58:37.234984Z","iopub.status.idle":"2021-07-30T06:58:37.261567Z","shell.execute_reply.started":"2021-07-30T06:58:37.234948Z","shell.execute_reply":"2021-07-30T06:58:37.259923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_lr = 1e-3\nepoch = 50\nweight_decay = 1e-6\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=weight_decay)\nsched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n                                            steps_per_epoch=len(train_loader))\n\nhistory = fit(epoch, model, train_loader, val_loader, optimizer, sched)","metadata":{"id":"5MXGppmwJC12","execution":{"iopub.status.busy":"2021-07-30T06:59:02.656293Z","iopub.execute_input":"2021-07-30T06:59:02.656689Z","iopub.status.idle":"2021-07-30T14:58:41.684793Z","shell.execute_reply.started":"2021-07-30T06:59:02.656656Z","shell.execute_reply":"2021-07-30T14:58:41.681225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = SegData(images, targets, X_val ,mean , std, transform = None , test = True)","metadata":{"id":"UxNA_Uj-JFxO","execution":{"iopub.status.busy":"2021-07-30T14:58:45.762085Z","iopub.execute_input":"2021-07-30T14:58:45.762442Z","iopub.status.idle":"2021-07-30T14:58:45.767354Z","shell.execute_reply.started":"2021-07-30T14:58:45.76241Z","shell.execute_reply":"2021-07-30T14:58:45.766334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image_mask(model, image , mask , mean=[0.485, 0.456, 0.406],\n                       std = [0.229 , 0.224 ,0.225]):\n  model.eval()\n  t= T.Compose([T.ToTensor() ,T.Normalize(mean, std)])\n  image = t(image)\n  model.to(device) ; image = image.to(device)\n  mask = mask.to(device)\n  with torch.no_grad():\n\n    image = image.unsqueeze(0)\n    mask = mask.unsqueeze(0)\n\n    output = model(image)\n    score = IoU(output, mask)\n    masked = torch.argmax(output , dim =1)\n    masked = masked.cpu().squeeze(0)\n  return masked , score","metadata":{"id":"vs8shrCzLIew","execution":{"iopub.status.busy":"2021-07-30T14:58:47.998958Z","iopub.execute_input":"2021-07-30T14:58:47.999308Z","iopub.status.idle":"2021-07-30T14:58:48.006382Z","shell.execute_reply.started":"2021-07-30T14:58:47.99927Z","shell.execute_reply":"2021-07-30T14:58:48.005241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfor i in range(0,10):\n  image , mask = test_set[i]\n  pred_mask , score = predict_image_mask(model , image , mask)\n  fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\n  ax1.imshow(image)\n  ax1.set_title('Picture');\n\n  ax2.imshow(mask)\n  ax2.set_title('Ground truth')\n  ax2.set_axis_off()\n\n  ax3.imshow(pred_mask)\n  ax3.set_title('TernausNet16 | IoU {:.3f}'.format(score))\n  ax3.set_axis_off()","metadata":{"id":"NJa6obJKIiDq","execution":{"iopub.status.busy":"2021-07-29T12:55:47.439018Z","iopub.status.idle":"2021-07-29T12:55:47.439651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_final_mask(model, image , mean=[0.485, 0.456, 0.406],\n                       std = [0.229 , 0.224 ,0.225]):\n  model.eval()\n  t= T.Compose([T.ToTensor() ,T.Normalize(mean, std)])\n  image = t(image)\n  model.to(device) ; image = image.to(device)\n  with torch.no_grad():\n\n    image = image.unsqueeze(0)\n\n    output = model(image)\n    masked = torch.argmax(output , dim =1)\n    masked = masked.cpu().squeeze(0)\n  return masked","metadata":{"id":"tw0yc_wTIkC5","execution":{"iopub.status.busy":"2021-07-30T14:58:51.615723Z","iopub.execute_input":"2021-07-30T14:58:51.616076Z","iopub.status.idle":"2021-07-30T14:58:51.622232Z","shell.execute_reply.started":"2021-07-30T14:58:51.616045Z","shell.execute_reply":"2021-07-30T14:58:51.621318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nfrom torchvision import transforms as T\n\n#cuda \ndevice =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef predict_final_mask(model, image , mean=[0.485, 0.456, 0.406],\n                       std = [0.229 , 0.224 ,0.225]):\n  model.eval()\n  t= T.Compose([T.ToTensor() ,T.Normalize(mean, std)])\n  image = t(image)\n  model.to(device) ; image = image.to(device)\n  with torch.no_grad():\n\n    image = image.unsqueeze(0)\n\n    output = model(image)\n    masked = torch.argmax(output , dim =1)\n    masked = masked.cpu().squeeze(0).numpy()\n  return masked\n\n\n\nmodel = torch.load('./model-0.932.pt')\n\nimg = cv2.imread('../input/test-data/mosaic_test.jpg')\ni=750\nj=750\nindex = 1\n\nfinal_output = np.zeros((3750,3750))\n\nwhile i <= 3750:\n    while j <=3750:\n        cropped_image = img[i-750:i, j-750:j]\n        im = cv2.cvtColor(cropped_image , cv2.COLOR_BGR2RGB)\n        kernel_sharp = np.array(([-2, -2, -2], [-2, 17, -2], [-2, -2, -2]), dtype='int')\n        im = cv2.filter2D(im, -1, kernel_sharp)\n        im = cv2.resize(im, (512 , 512) , interpolation = cv2.INTER_NEAREST)\n        masked_image = predict_final_mask(model,im)\n        path = os.path.join('./output/','%s.png'%index)\n        masked_image = cv2.resize(masked_image,(750,750),interpolation = cv2.INTER_NEAREST)\n        final_output[i-750:i, j-750:j] = masked_image\n        j=j+750\n        index=index+1\n    i=i+750\n    j=750\nplt.figure(figsize = (15,12) )\nplt.imshow(final_output)\ncv2.imwrite('./output.png',final_output)\nnp.save('./out_imds.npy',final_output)","metadata":{"id":"oy2siMNAJuWW","execution":{"iopub.status.busy":"2021-07-30T15:03:44.207379Z","iopub.execute_input":"2021-07-30T15:03:44.207772Z","iopub.status.idle":"2021-07-30T15:03:49.74964Z","shell.execute_reply.started":"2021-07-30T15:03:44.207732Z","shell.execute_reply":"2021-07-30T15:03:49.748857Z"},"trusted":true},"execution_count":null,"outputs":[]}]}